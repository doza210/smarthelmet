{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL FOR IMU NEURAL NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GETTING Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAwake_train = pd.read_csv('Data/training_AData.txt', header=None, delim_whitespace=True)\n",
    "dfDrowsy_train = pd.read_csv('Data/training_DData.txt', header=None, delim_whitespace=True)\n",
    "dfAwake_test = pd.read_csv('Data/testing_AData.txt', header=None, delim_whitespace=True)\n",
    "dfDrowsy_test = pd.read_csv('Data/testing_DData.txt', header=None, delim_whitespace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Retrieved Data informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240000, 15)\n",
      "(3240000, 15)\n",
      "(1620000, 15)\n",
      "(1620000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(dfAwake_train.shape)\n",
    "print(dfDrowsy_train.shape)\n",
    "print(dfAwake_test.shape)\n",
    "print(dfDrowsy_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Not Needed Columns Keeping IMU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAwake_train = dfAwake_train.drop(14,axis = 1)\n",
    "dfAwake_train = dfAwake_train.drop(dfAwake_train.iloc[:,0:12],axis = 1)\n",
    "dfDrowsy_train = dfDrowsy_train.drop(14,axis = 1)\n",
    "dfDrowsy_train = dfDrowsy_train.drop(dfDrowsy_train.iloc[:,0:12],axis = 1)\n",
    "dfAwake_test = dfAwake_test.drop(14,axis = 1)\n",
    "dfAwake_test = dfAwake_test.drop(dfAwake_test.iloc[:,0:12],axis = 1)\n",
    "dfDrowsy_test = dfDrowsy_test.drop(14,axis = 1)\n",
    "dfDrowsy_test = dfDrowsy_test.drop(dfDrowsy_test.iloc[:,0:12],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3240000, 2)\n",
      "(3240000, 2)\n",
      "(1620000, 2)\n",
      "(1620000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(dfAwake_train.shape)\n",
    "print(dfDrowsy_train.shape)\n",
    "print(dfAwake_test.shape)\n",
    "print(dfDrowsy_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESHAPING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 900, 2)\n",
      "(3600, 900, 2)\n",
      "(1800, 900, 2)\n",
      "(1800, 900, 2)\n"
     ]
    }
   ],
   "source": [
    "dfAwake_train = dfAwake_train.to_numpy()\n",
    "dfDrowsy_train = dfDrowsy_train.to_numpy()\n",
    "dfAwake_train = dfAwake_train.reshape(3600,900,2)\n",
    "dfDrowsy_train = dfDrowsy_train.reshape(3600,900,2)\n",
    "dfAwake_test = dfAwake_test.to_numpy()\n",
    "dfDrowsy_test = dfDrowsy_test.to_numpy()\n",
    "dfAwake_test = dfAwake_test.reshape(1800,900,2)\n",
    "dfDrowsy_test = dfDrowsy_test.reshape(1800,900,2)\n",
    "print(dfAwake_train.shape)\n",
    "print(dfDrowsy_train.shape)\n",
    "print(dfAwake_test.shape)\n",
    "print(dfDrowsy_test.shape)\n",
    "# print(dfAwake.head)\n",
    "# print(dfDrowsy.head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATING LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600,)\n",
      "(3600,)\n",
      "(1800,)\n",
      "(1800,)\n"
     ]
    }
   ],
   "source": [
    "dfAwake_trainLabels = np.zeros(3600)\n",
    "print(dfAwake_trainLabels.shape)\n",
    "dfDrowsy_trainLabels = np.ones(3600)\n",
    "print(dfDrowsy_trainLabels.shape)\n",
    "dfAwake_testLabels = np.zeros(1800)\n",
    "print(dfAwake_testLabels.shape)\n",
    "dfDrowsy_testLabels = np.ones(1800)\n",
    "print(dfDrowsy_testLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dfAwake_trainLabels)\n",
    "print(dfDrowsy_trainLabels)\n",
    "print(dfAwake_testLabels)\n",
    "print(dfDrowsy_testLabels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Awake Data and Drowsy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 900, 2)\n",
      "(7200,)\n",
      "[[[-0.01347178 -0.00392533]\n",
      "  [ 0.00561919  0.00493475]\n",
      "  [ 0.02176988  0.00053666]\n",
      "  ...\n",
      "  [ 0.01136684 -0.00309513]\n",
      "  [ 0.01403857 -0.00426882]\n",
      "  [ 0.02121865 -0.00393233]]\n",
      "\n",
      " [[ 0.00561919  0.00493475]\n",
      "  [ 0.02176988  0.00053666]\n",
      "  [ 0.00405722  0.01380083]\n",
      "  ...\n",
      "  [ 0.01403857 -0.00426882]\n",
      "  [ 0.02121865 -0.00393233]\n",
      "  [ 0.00454181 -0.01287709]]\n",
      "\n",
      " [[ 0.02176988  0.00053666]\n",
      "  [ 0.00405722  0.01380083]\n",
      "  [ 0.00771384  0.00374309]\n",
      "  ...\n",
      "  [ 0.02121865 -0.00393233]\n",
      "  [ 0.00454181 -0.01287709]\n",
      "  [-0.00323185 -0.00783903]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.00226274  0.00575061]\n",
      "  [ 0.0075034  -0.00658379]\n",
      "  [ 0.00472974 -0.00671845]\n",
      "  ...\n",
      "  [ 0.00261806  0.00260904]\n",
      "  [-0.00574393  0.00689247]\n",
      "  [-0.00291118 -0.00434906]]\n",
      "\n",
      " [[ 0.0075034  -0.00658379]\n",
      "  [ 0.00472974 -0.00671845]\n",
      "  [-0.01043967  0.00304691]\n",
      "  ...\n",
      "  [-0.00574393  0.00689247]\n",
      "  [-0.00291118 -0.00434906]\n",
      "  [ 0.0099463   0.00430073]]\n",
      "\n",
      " [[ 0.00472974 -0.00671845]\n",
      "  [-0.01043967  0.00304691]\n",
      "  [-0.00011717 -0.00286729]\n",
      "  ...\n",
      "  [-0.00291118 -0.00434906]\n",
      "  [ 0.0099463   0.00430073]\n",
      "  [-0.00335484 -0.0116052 ]]]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "dftrain = np.concatenate((dfAwake_train, dfDrowsy_train))\n",
    "dftrainlabels =np.concatenate((dfAwake_trainLabels, dfDrowsy_trainLabels))\n",
    "print(dftrain.shape)\n",
    "print(dftrainlabels.shape)\n",
    "print(dftrain)\n",
    "print(dftrainlabels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHUFFLING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3518 7112 5983 ... 4343 3757 7161]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.permutation(len(dftrainlabels))\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain=dftrain[idx]\n",
    "dftrainlabels=dftrainlabels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 900, 2)\n",
      "(7200,)\n",
      "[[[-2.90092639e-03 -2.79204333e-03]\n",
      "  [-3.30853802e-03 -1.30947933e-03]\n",
      "  [ 9.43122306e-03 -1.04934558e-02]\n",
      "  ...\n",
      "  [-1.28770645e-03 -1.24637890e-02]\n",
      "  [ 1.49313893e-02  1.71731082e-02]\n",
      "  [ 9.09639256e-03  4.18441115e-03]]\n",
      "\n",
      " [[-3.62938499e-03 -2.67135525e-03]\n",
      "  [-4.69024132e-03  2.23973078e-03]\n",
      "  [-4.87973737e-03  8.45971046e-03]\n",
      "  ...\n",
      "  [-1.35397010e-03  7.48637634e-03]\n",
      "  [-1.89825274e-03 -4.34819060e-03]\n",
      "  [ 3.23767986e-03  8.27991046e-03]]\n",
      "\n",
      " [[ 2.04966944e-03 -5.09992131e-03]\n",
      "  [ 3.95544318e-03  1.91967999e-03]\n",
      "  [-2.12188333e-03  9.64311128e-03]\n",
      "  ...\n",
      "  [-7.50871505e-02  3.24829239e-02]\n",
      "  [-2.57322321e-02 -1.27238666e-01]\n",
      "  [ 9.01524417e-03 -2.81105568e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.96369740e-02 -6.82038339e-02]\n",
      "  [-3.92838959e-02 -5.34148932e-02]\n",
      "  [-2.50104406e-02 -2.93740584e-02]\n",
      "  ...\n",
      "  [-1.95138201e-02  1.29840729e-02]\n",
      "  [-1.57278366e-03  1.32758077e-02]\n",
      "  [-2.29421074e-04 -5.26506417e-03]]\n",
      "\n",
      " [[ 6.67564242e-02 -4.17301544e-02]\n",
      "  [ 8.01209341e-02  7.24629362e-03]\n",
      "  [ 3.29636075e-02 -1.88150550e-02]\n",
      "  ...\n",
      "  [-4.29569712e-03 -9.73903916e-03]\n",
      "  [ 4.07024343e-03 -1.51911558e-02]\n",
      "  [ 6.53804879e-03  3.59053541e-03]]\n",
      "\n",
      " [[ 3.42738945e-03 -6.69199325e-04]\n",
      "  [ 6.31215905e-04 -9.24037745e-03]\n",
      "  [ 6.20480826e-04 -1.07246479e-02]\n",
      "  ...\n",
      "  [ 5.64143525e-03  1.61716737e-02]\n",
      "  [ 4.21605223e-04  4.43324295e-03]\n",
      "  [-1.26692638e-02  4.19774364e-03]]]\n",
      "[0. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(dftrain.shape)\n",
    "print(dftrainlabels.shape)\n",
    "print(dftrain)\n",
    "print(dftrainlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 900, 2)\n",
      "(3600,)\n",
      "[[[ 0.02311828  0.011094  ]\n",
      "  [ 0.03492036 -0.01857721]\n",
      "  [ 0.02784405 -0.01861933]\n",
      "  ...\n",
      "  [ 0.01446212 -0.01794756]\n",
      "  [ 0.01531532 -0.01282402]\n",
      "  [ 0.00547928 -0.0126411 ]]\n",
      "\n",
      " [[ 0.03492036 -0.01857721]\n",
      "  [ 0.02784405 -0.01861933]\n",
      "  [ 0.00261633 -0.02434868]\n",
      "  ...\n",
      "  [ 0.01531532 -0.01282402]\n",
      "  [ 0.00547928 -0.0126411 ]\n",
      "  [-0.00173669 -0.01653759]]\n",
      "\n",
      " [[ 0.02784405 -0.01861933]\n",
      "  [ 0.00261633 -0.02434868]\n",
      "  [ 0.01006408 -0.01847897]\n",
      "  ...\n",
      "  [ 0.00547928 -0.0126411 ]\n",
      "  [-0.00173669 -0.01653759]\n",
      "  [ 0.01429986  0.03591099]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.01538968 -0.00381893]\n",
      "  [-0.01108513  0.00549479]\n",
      "  [-0.0132084  -0.00062503]\n",
      "  ...\n",
      "  [-0.00202253  0.0016088 ]\n",
      "  [-0.00207898 -0.00485158]\n",
      "  [ 0.00513121 -0.00311212]]\n",
      "\n",
      " [[-0.01108513  0.00549479]\n",
      "  [-0.0132084  -0.00062503]\n",
      "  [-0.01549576 -0.01200462]\n",
      "  ...\n",
      "  [-0.00207898 -0.00485158]\n",
      "  [ 0.00513121 -0.00311212]\n",
      "  [ 0.01296438 -0.00488806]]\n",
      "\n",
      " [[-0.0132084  -0.00062503]\n",
      "  [-0.01549576 -0.01200462]\n",
      "  [-0.01053267  0.00184589]\n",
      "  ...\n",
      "  [ 0.00513121 -0.00311212]\n",
      "  [ 0.01296438 -0.00488806]\n",
      "  [ 0.00954596 -0.01792024]]]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "dftest = np.concatenate((dfAwake_test, dfDrowsy_test))\n",
    "dftestlabels =np.concatenate((dfAwake_testLabels, dfDrowsy_testLabels))\n",
    "print(dftest.shape)\n",
    "print(dftestlabels.shape)\n",
    "print(dftest)\n",
    "print(dftestlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 900, 2)\n",
      "(3600,)\n",
      "(7200, 900, 2)\n",
      "(7200,)\n"
     ]
    }
   ],
   "source": [
    "print(dftest.shape)\n",
    "print(dftestlabels.shape)\n",
    "print(dftrain.shape)\n",
    "print(dftrainlabels.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input and output dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dftrain shape: (7200, 900, 2)\n",
      "input_shape: 1800\n"
     ]
    }
   ],
   "source": [
    "input_shape = 900*2\n",
    "num_classes = 2\n",
    "# dftrain = dftrain.reshape(dftrain.shape[0],input_shape)\n",
    "print('dftrain shape:', dftrain.shape)\n",
    "print('input_shape:', input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = dftrain.astype('float32')\n",
    "dftrainlabels = dftrainlabels.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dfttrainlabels shape:  [[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "dftrainlabels_hot = np_utils.to_categorical(dftrainlabels,num_classes)\n",
    "print('New dfttrainlabels shape: ', dftrainlabels_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dfttrainlabels shape:  (7200, 2)\n",
      "[0. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print('New dfttrainlabels shape: ', dftrainlabels_hot.shape)\n",
    "print(dftrainlabels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 2ms/step - loss: 0.4966 - sparse_categorical_accuracy: 0.7350\n",
      "Test accuracy 0.7350000143051147\n",
      "Test loss 0.4965668022632599\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(\"best_model(imuv3).h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(dftrain, dftrainlabels,verbose=1)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_m = Sequential()\n",
    "# # Remark: since coreml cannot accept vector shapes of complex shape like\n",
    "# # [80,3] this workaround is used in order to reshape the vector internally\n",
    "# # prior feeding it into the network\n",
    "# model_m.add(Reshape((900, 2), input_shape=(input_shape,)))\n",
    "# model_m.add(Dense(100, activation='relu'))\n",
    "# # model_m.add(BatchNormalization()) # V2 addition\n",
    "# model_m.add(Dense(100, activation='relu'))\n",
    "# # model_m.add(BatchNormalization()) # V2 addition\n",
    "# model_m.add(Dense(100, activation='relu'))\n",
    "# # model_m.add(BatchNormalization()) # V2 addition \n",
    "# model_m.add(Flatten())\n",
    "# model_m.add(Dense(num_classes, activation='softmax'))\n",
    "# print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.ReLU()(conv1)\n",
    "\n",
    "    # conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n",
    "    # conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    # conv2 = keras.layers.ReLU()(conv2)\n",
    "\n",
    "    # conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n",
    "    # conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    # conv3 = keras.layers.ReLU()(conv3)\n",
    "\n",
    "    gap = keras.layers.GlobalAveragePooling1D()(conv1)\n",
    "\n",
    "    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n",
    "\n",
    "    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=dftrain.shape[1:])\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FITTING Deep neural network Model in KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks_list = [\n",
    "#     keras.callbacks.ModelCheckpoint(\n",
    "#         filepath='best_model(imu).{epoch:02d}-{val_loss:.2f}.h5',\n",
    "#         monitor='val_loss', save_best_only=True),\n",
    "#     keras.callbacks.EarlyStopping(monitor='accuracy', patience=1)\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# model_m.compile(loss='categorical_crossentropy',\n",
    "#                 optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "\n",
    "# # Hyper-parameters\n",
    "# BATCH_SIZE = 400 # V3 changed to 2160\n",
    "# EPOCHS = 50\n",
    "\n",
    "\n",
    "# # Enable validation to use ModelCheckpoint and EarlyStopping callbacks.\n",
    "# # v1\n",
    "# history = model_m.fit(dftrain,\n",
    "#                       dftrainlabels_hot,\n",
    "#                       batch_size=BATCH_SIZE,\n",
    "#                       epochs=EPOCHS,\n",
    "#                       callbacks=callbacks_list,\n",
    "#                       validation_split=0.2,\n",
    "#                       verbose=1)\n",
    "\n",
    "# # # v1 completing all epochs\n",
    "# # history = model_m.fit(dftrain,\n",
    "# #                       dftrainlabels_hot,\n",
    "# #                       batch_size=BATCH_SIZE,\n",
    "# #                       epochs=EPOCHS,\n",
    "# #                       validation_split=0.2,\n",
    "# #                       verbose=1)\n",
    "\n",
    "# # v3\n",
    "# # history = model_m.fit(dftrain,\n",
    "# #                       dftrainlabels_hot,\n",
    "# #                       batch_size=BATCH_SIZE,\n",
    "# #                       epochs=EPOCHS,\n",
    "# #                       shuffle=True,\n",
    "# #                       validation_split=0.2,\n",
    "# #                       verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 288\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        \"best_model(imu).h5\", save_best_only=True, monitor=\"val_loss\"\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=5, min_lr=0.0001\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=\"categorical_crossentropy\",\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "history = model.fit(\n",
    "    dftrain,\n",
    "    dftrainlabels,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.4,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 4))\n",
    "# plt.plot(history.history['accuracy'], 'r', label='Accuracy of training data')\n",
    "# plt.plot(history.history['val_accuracy'], 'b', label='Accuracy of validation data')\n",
    "# plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
    "# plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
    "# plt.title('Model Accuracy and Loss')\n",
    "# plt.ylabel('Accuracy and Loss')\n",
    "# plt.xlabel('Training Epoch')\n",
    "# plt.ylim(0)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "metric = \"sparse_categorical_accuracy\"\n",
    "plt.figure()\n",
    "plt.plot(history.history[metric])\n",
    "plt.plot(history.history[\"val_\" + metric])\n",
    "plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
    "plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
    "plt.title(\"model \" + metric)\n",
    "plt.ylabel(metric, fontsize=\"large\")\n",
    "plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "plt.legend([\"train\", \"val\",\"loss\",\"val_loss\"], loc=\"best\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Print confusion matrix for training data\n",
    "y_pred_train = model.predict(dftrain)\n",
    "# Take the class with the highest probability from the train predictions\n",
    "max_y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "print(classification_report(dftrainlabels, max_y_pred_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking against Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set input_shape / reshape for Keras\n",
    "# dftest = dftest.reshape(dftest.shape[0], input_shape)\n",
    "\n",
    "dftest = dftest.astype('float32')\n",
    "dftestlabels = dftestlabels.astype('float32')\n",
    "\n",
    "dftestlabels_hot = np_utils.to_categorical(dftestlabels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('New dfttrainlabels shape: ', dftestlabels_hot)\n",
    "print('New dfttrainlabels shape: ', dftestlabels.shape)\n",
    "print('New dfttrainlabels shape: ', dftestlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score = model_m.evaluate(dftest, dftestlabels_hot, verbose=1)\n",
    "\n",
    "# print('\\nAccuracy on test data: %0.2f' % score[1])\n",
    "# print('\\nLoss on test data: %0.2f' % score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['Awake','Drowsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = \"sparse_categorical_accuracy\"\n",
    "# plt.figure()\n",
    "# plt.plot(history.history[metric])\n",
    "# plt.plot(history.history[\"val_\" + metric])\n",
    "# plt.title(\"model \" + metric)\n",
    "# plt.ylabel(metric, fontsize=\"large\")\n",
    "# plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "# plt.legend([\"train\", \"val\"], loc=\"best\")\n",
    "# plt.show()\n",
    "# plt.close()s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"best_model(imuv3).h5\")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(dftest, dftestlabels,verbose=1)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print confusion matrix for training data\n",
    "# y_pred_train = model.predict(dftest)\n",
    "# # Take the class with the highest probability from the train predictions\n",
    "# max_y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "# print(classification_report(dftestlabels, max_y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "y_pred_test = model.predict(dftest)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(dftestlabels_hot, axis=1)\n",
    "\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
